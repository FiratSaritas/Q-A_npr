# Q-A_npr
In this Repo, three different models are compared for question answering. To train the models the data set \href{https://rajpurkar.github.io/SQuAD-explorer/}{Squad-V1} was used. For the comparison an individual collection of 15 texts and 3 questions per text was created. Eventually, to measure the performance of the models, the F1-score was used of tokenized output answers as the evaluation metric. In our analysis it was visible that the model output performances depended on the difficulties of the questions. Further we could state that the model performances were not stable to unanswerable questions, long and multi-span answers and the interpretation of the given context. In the evaluation one could see that the most heavy model Roberta performed best throughout all text and question difficulties.
